# ğŸ•¸ï¸ GitHub Crawler

## ğŸ“Œ Overview
This project is a **GitHub Crawler** that uses **GitHubâ€™s GraphQL API** to collect repository data â€” specifically repository names and their star counts.  
The data is stored in a database and exported to a `.csv` file.  
The workflow is fully automated using **GitHub Actions**.

---

## âš™ï¸ Features
- Uses **GitHub GraphQL API** to fetch repositories
- Stores results in a database (`repos.db`)
- Exports output to a CSV file (`repos.csv`)
- Runs automatically through **GitHub Actions**
- Uses the **default GitHub Token** (no private secrets required)

---

## ğŸ—‚ï¸ Project Structure
github-crawler/
â”‚
â”œâ”€â”€ crawler/
â”‚ â”œâ”€â”€ main.py # Main script to run the crawler
â”‚ â”œâ”€â”€ db.py # Database schema and data insertion logic
â”‚ â””â”€â”€ github_api.py # GitHub GraphQL API request handler
â”‚
â”œâ”€â”€ repos.db # Database storing repository data
â”œâ”€â”€ repos.csv # Exported repository data
â””â”€â”€ .github/


---

## ğŸš€ How It Works
1. The GitHub Action sets up Python and installs dependencies.  
2. It runs `crawler/main.py` which:
   - Fetches repositories and their star counts
   - Stores data in `repos.db`
   - Exports results to `repos.csv`
3. The workflow completes automatically and uploads the output as an artifact.

---

## âœ… Workflow Status
The workflow runs successfully in GitHub Actions.  
Latest run: **Succeeded âœ…**

---

## ğŸ‘©â€ğŸ’» Author
**Marium Zehra**  
GitHub: [@marium17](https://github.com/marium17)

â””â”€â”€ workflows/
â””â”€â”€ main.yml # GitHub Actions workflow file
